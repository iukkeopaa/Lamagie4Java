到目前为止，我们分析完了分布式锁的全部核心源码和思想，很详细的介绍了Redis和Zookeeper实现分布式锁的核心原理以及核心架构，上一篇是Zookeeper系列的精华，必须要看明白。本篇内容为补充篇，并不是实现分布式锁，而是用分布式锁的设计思想来实现一个分布式限流工具，接下来我不会从手写一个分布式限流工具，而是我会剖析其核心设计思想以及看看如何用Zookeeper进行实现。本篇会先介绍背景，也就是需求描述，然后对需求进行方案设计，最后我们会对照方案设计进行核心源码的实现。

# 一、背景

什么是限流 ？就是限制同一方法同时最大有多少个线程访问。比如：

我们做了个IM即时通讯系统，最大支持同时1000人在线聊天，但是我们IM上线后很火爆，有十万个人都想用，但是我们系统明显无法支撑同时10万个人在线聊天，那怎么办呢？总不能一上线后服务就被压垮了吧。这就需要限流了，我限制最多同时1000人在线聊天。多的人就排队，无法登陆。

限流是什么意思应该已经明白了，限流又分为两种：单机限流和分布式限流。单机和分布式这两个概念到目前为止应该不再陌生，单机就是单节点（Java的话就是单JVM）能支撑同时多少人在线，而分布式就是整个集群内能支撑同时多少人在线。

现在背景明白了，那就开始想想该如何实现吧。

# 二、方案

在实现之前要先进行方案设计，不知道你是否还记得一个东西：Semaphore。这个东西在JDK里也有，所以对这个概念应该不陌生，就是同时允许多少个客户端持有锁，那你想想，这个东西是不是恰巧能解决分布式限流工具？我们需求背景是限制同时并发数，然后Semaphore这个东西恰巧能做到同时允许多少个客户端持有锁，这不就是完全吻合我们的需求嘛。但是如何实现一个分布式的Semaphore呢？接下来我会采取两种方式进行实现，一种是利用Redis来实现，另一种是利用Zookeeper来实现，但是我不会从零开始手写，因为Redisson和Curator已经为我们提供了，所以我会剖析其核心思想以及带着思想深入到源码细节当中。话不多说，进入正题，看看Zookeeper如何实现分布式限流工具的～

# 三、Curator#Semaphore

**首先我们肯定需要具备一个参数：这个参数就是同时允许多少个客户端加锁？** 所以我们可以定义一个变量，然后通过构造器进行设置值。

**其次我们需要考虑一件事：如何表示抢占锁成功这件事？** 也就是说客户端抢占锁成功了，Zookeeper该如何表示？这个想必就很简单了，因为上篇把Zookeeper实现分布式锁的核心原理剖析的淋漓尽致了，它就是**临时顺序节点**，每抢占成功一个就创建一个临时顺序节点。如果当前加锁的个数没超出限制的个数的话，那就继续执行业务流程，如果当前加锁的个数超出了的话，那么就阻塞，然后释放的时候进行唤醒。

那如何获取当前加锁的个数？提供一个getChildren()方法，统计锁目录下的文件（临时顺序节点）个数。

**最后加锁客户端数量超出我们限制数量的那部分是阻塞等待释放后立即抢占锁还是直接丢弃？** 直接丢弃太不友好，我们采取阻塞等待的方式，然后释放锁的时候重新抢占，可以搞个死循环，超出了就wait()住，释放就notifyAll()，然后wait()不在阻塞，进入下一次循环尝试抢锁。

现在实现方案以及如何实现都已经明白了，我们根据上面刚讲的实现细节一步步的看源码。先来看第一个：**同时允许多少个客户端加锁？** 这个参数是怎么设计的？

## 1\. 同时允许多少个客户端加锁

```java
// org.apache.curator.framework.recipes.locks.InterProcessSemaphoreV2#InterProcessSemaphoreV2(org.apache.curator.framework.CuratorFramework, java.lang.String, int, org.apache.curator.framework.recipes.shared.SharedCountReader)
​
// 省略了其他代码
private InterProcessSemaphoreV2(CuratorFramework client, String path, int maxLeases, SharedCountReader count) {
    this.maxLeases = (count != null) ? count.getCount() : maxLeases;
}
```

maxLeases是什么？他就是同时允许多少个客户端加锁的核心参数。好了，我们知道这个参数叫maxLeases，且是通过构造器让用户来自己传递的，完全符合我们的设计，那就继续往下看，**我们如何表示抢占锁成功这件事？** 简单来说就是：**如何实现同时允许多个客户端加锁？**

## 2\. 如何实现同时允许多客户端加锁？

首先我们能确定的是：**加锁就是创建临时顺序节点**，如下代码，很简单：

```java
// org.apache.curator.framework.recipes.locks.InterProcessSemaphoreV2#internalAcquire1Lease
​
// 省略了其他代码
private InternalAcquireResult internalAcquire1Lease(ImmutableList.Builder<Lease> builder, long startMs, boolean hasWait, long waitMs) throws Exception {
    // 创建临时节点
    PathAndBytesable<String> createBuilder = client
        .create()
        .creatingParentContainersIfNeeded()
        .withProtection()
        .withMode(CreateMode.EPHEMERAL_SEQUENTIAL);
}
```

现在临时节点有了也就代表加锁成功了，那怎么允许多个客户端加锁？这还不简单吗？我们都已经知道最大能同时有几个客户端加锁了（构造器传进来的maxLeases），那判断下不就完了？判断条件是当前上锁的客户端数量小于等于maxLeases的话，就允许正常加锁。

```java
// org.apache.curator.framework.recipes.locks.InterProcessSemaphoreV2#internalAcquire1Lease
​
// 省略了其他代码
private InternalAcquireResult internalAcquire1Lease(ImmutableList.Builder<Lease> builder, long startMs, boolean hasWait, long waitMs) throws Exception {
    // 省略上面创建临时节点的代码
    // ...
    
    for(;;) {
        // 获取所有已经加锁的客户端数量，实际是已经创建了临时节点的数量。
        children = client.getChildren().usingWatcher(watcher).forPath(leasesPath);
        // 如果已经加锁的客户端数量小于等于允许数量。那直接return放行。
        if ( children.size() <= maxLeases) {
            break ;
        }
    }
}
```

现在我们已经知道了如何多客户端同时加锁，核心就在于一个判断，那如果已经加锁的客户端数量大于允许加锁的数量怎么办？也就是对应我们设计思想里的最后一点，**阻塞等待释放锁，这是如何做的？**

## 3\. 超过限制如何阻塞等待？

想必大家都猜到了答案，上面判断小于等于，我加个`else {wait();}`不就完事了吗？是的，就是这样，所以完整代码如下

```java
// org.apache.curator.framework.recipes.locks.InterProcessSemaphoreV2#internalAcquire1Lease
​
// 省略了其他代码
private InternalAcquireResult internalAcquire1Lease(ImmutableList.Builder<Lease> builder, long startMs, boolean hasWait, long waitMs) throws Exception {
    // 创建临时节点
    PathAndBytesable<String> createBuilder = client
        .create()
        .creatingParentContainersIfNeeded()
        .withProtection()
        .withMode(CreateMode.EPHEMERAL_SEQUENTIAL);
    
    for(;;) {
        // 获取所有已经加锁的客户端数量，实际是已经创建了临时节点的数量。
        children = client.getChildren().usingWatcher(watcher).forPath(leasesPath);
        // 如果已经加锁的客户端数量小于等于允许数量。那直接return放行。
        if ( children.size() <= maxLeases) {
            break ;
        }
        // 阻塞等待。
        wait();
    }
}
```

![zk-semaphore-lock.png](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/1be2316a12b84f6f9725baf04021b238~tplv-k3u1fbpfcp-jj-mark:1600:0:0:0:q75.image#?w=1864&h=600&s=84438&e=png&b=ffffff) 相当于加锁的时候，如果超出了限制，就阻塞等待，这样方法就会被阻塞住，也就相当于业务逻辑没得到执行。但是一直阻塞吗？肯定不是，肯定是有个唤醒的时机，那**什么时候唤醒**呢？

## 4\. 释放唤醒

当然是有其他客户端释放锁的时候来唤醒阻塞等待的客户端，然后被唤醒的客户端结束阻塞，重新进入循环尝试加锁。那是如何释放的呢？

```java
// org.apache.curator.framework.recipes.locks.InterProcessSemaphoreV2#makeLease
​
// 省略其他代码
private Lease makeLease(final String path) {
    return new Lease() {
        @Override
        public void close() throws IOException {
            try {
                // 直接删除临时顺序节点
                client.delete().guaranteed().forPath(path);
            }
        }
    };
}
```

奇怪，只看到了删除临时顺序节点，没有看到唤醒的操作啊，返回去看我们获取children的方法：

`children = client.getChildren().usingWatcher(watcher).forPath(leasesPath);`

看到了没，他注册了个监听器，watcher，那watcher是什么呢？

```java
public class InterProcessSemaphoreV2 {
    private final Watcher watcher = new Watcher() {
        @Override
        public void process(WatchedEvent event) {
            client.postSafeNotify(InterProcessSemaphoreV2.this);
        }
    };
}
​
// org.apache.curator.framework.CuratorFramework#postSafeNotify
default CompletableFuture<Void> postSafeNotify(Object monitorHolder) {
    return runSafe(() -> {
        synchronized(monitorHolder) {
            // notifyAll(); !!!
            monitorHolder.notifyAll();
        }
    });
}
```

这次应该清晰了吧？加锁的时候开启watcher监听临时顺序节点，然后释放锁的时候会删除临时顺序节点，删除后，监听器会收到通知，执行notifyAll()唤醒其他wait()的线程。唤醒后不就又进入for(;;)继续尝试加锁了嘛。

![zk-semaphore-unlock.png](https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/3af3353435024fbba87a26b2a366d778~tplv-k3u1fbpfcp-jj-mark:1600:0:0:0:q75.image#?w=1664&h=244&s=46678&e=png&b=ffffff) 全过程到此结束了，但是有没有发现一个问题：线程不安全的问题。

加锁的顺序是：创建临时节点、判断数量是否超出限制然后选择是放行还是wait()阻塞等待。这一步骤不是原子的啊！假设我最大同时允许3个客户端持有锁，那么可能发生的情况是：

有4个客户端都请求加锁方法了，然后4个节点都创建了临时节点，最后getChildren()的时候都是4，但是却发现4<3，全部走了wait()方法，这相当于我们没有一个客户端加锁成功，全部阻塞住了，而且是死锁了，永远得不到释放。

出现问题的关键在于创建临时节点和判断数量是否超出限制这两个操作不是原子操作，那我们就把它变成原子操作不就好了吗？怎么实现？外层再包一层互斥锁，也就是我们上篇幅分析的锁！一起来看下具体的源码设计

## 5\. 双锁

```java
// org.apache.curator.framework.recipes.locks.InterProcessSemaphoreV2#internalAcquire1Lease
​
// 上一篇刚讲的公平可重入锁。
private final InterProcessMutex lock;
​
// 省略了其他代码
private InternalAcquireResult internalAcquire1Lease(ImmutableList.Builder<Lease> builder, long startMs, boolean hasWait, long waitMs) throws Exception {
    // 重点在这了，在执行下面操作之前，我先上个锁来保证下面的原子性操作。
    lock.acquire();
    
    try {
        // 创建临时节点
        // ...
​
        for(;;) {
            // 获取所有已经加锁的客户端数量，实际是已经创建了临时节点的数量。
            // 如果已经加锁的客户端数量小于等于允许数量。那直接return放行。
            // 否则阻塞等待。
        }
    } finally {
        // 释放互斥锁。如果上面是wait()阻塞了，是走不到finally的。如果如果加锁成功的话就释放互斥锁，这样下一个client就又能进来了
        lock.release();
    }
}
```

很简单也很清晰明了，就是在执行那段非原子操作（创建临时节点和判断数量这两步）之前上把互斥锁来保证原子性。现在在分析下流程，看看是否还存在问题：

**首先客户端1申请加锁的时候先上个互斥锁，然后进行创建Semaphore允许多客户端共存的锁的临时节点，获取当前存在的Semaphore锁数量，如果没超出限制，那就正常break掉，然后走到finally释放掉互斥锁，这样其他客户端就能抢锁了。那如果超出了限制呢？那就会wait()住，走不到finally，无法释放互斥锁，其他客户端就只能排队等待，无法竞争。**

**那如果客户端1还没执行完流程，客户端2来申请加锁了会怎样？客户端2会阻塞排队且监听客户端1的临时顺序节点，这样客户端1释放后（也就是删除临时顺序节点后），客户端2回收到通知，然后进行notifyAll()唤醒，重新抢锁。（这个流程上一篇讲的很清晰啦）**

![zk-semaphore-lock-all.png](https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/71d34766c0264a8fb717879d725c97bc~tplv-k3u1fbpfcp-jj-mark:1600:0:0:0:q75.image#?w=2132&h=1104&s=141781&e=png&b=ffffff) 这套流程不仅完美的解决了原子性的问题，还提供了如下好处：

**能保证绝对的公平性，下一个节点监听上一个节点，井然有序，而且释放锁的时候也不必通知全部，释放锁后下一个节点的监听器就会收到通知，然后自行notifyAll()进行唤醒。**

Zookeeper实现分布式限流工具的核心内容到此结束了，我们来总结下。

## 6\. 总结

要重点掌握Curator#Semaphore的**双锁机制**，很巧妙。同时要掌握加锁解锁的核心原理。我们用一张图来总结下：

![zk-semaphore-all.png](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/7eb6149aa2a24e53b02c53d3d8e78d81~tplv-k3u1fbpfcp-jj-mark:1600:0:0:0:q75.image#?w=2156&h=1610&s=349153&e=png&b=fefefe) 最后补充个知识：不知道你们有没有发现Semaphore是不可重入的，因为代码里完全没有重入的逻辑，我们上一篇讲解的是可重入的公平互斥锁，有个LockData对象，里面有个整形原子类来存储重入次数。那我现在问你：**如何设计一个不可重入互斥锁？**

这个问题你肯定会想到一个方案：继承下可重入的公平互斥锁，然后重写下重入的逻辑不就行了？对不起，他是private的，也不是模板设计模式，你根本没法重写这部分逻辑，除非你复制粘贴一份全部重做。

那Zookeeper不能快速实现不可重入的互斥分布式锁了吗？他的设计这么不可扩展吗？

你自己想想我们的Semaphore，允许多个客户端同时持有锁，不可重入的，个数自定义。你仔细品这句话，我们不可重入锁是不是只要把个数传递1，这事就解决了？

也就变成了最多只能1个客户端持有锁，这个锁还是不可重入的，完美符合预期。

Curator也是这么做的，源码在`org.apache.curator.framework.recipes.locks.InterProcessSemaphoreMutex`

```arduino
public InterProcessSemaphoreMutex(CuratorFramework client, String path) {
    // new Semaphore(1)，最多只能允许一个客户端持有锁。
    this.semaphore = new InterProcessSemaphoreV2(watcherRemoveClient, path, 1);
}
```

最后的最后留个作业吧，我们此篇分析了分布式限流工具的背景以及代码实现，但是我们剖析的是Zookeeper，其实Redis也能实现，而且Redisson也已经为我们实现好了，所以这里的作业是大家自己看看Redisson是如何实现分布式限流工具的？大家一定要养成习惯：我先不看源码，先结合自己之前所学的Redis知识进行方案设计，然后再看看源码，看看你的方案和源码实现的差距，还是那句话，看源码不是学源码，而是学思想。Redisson实现分布式限流的类如下：`RedissonSemaphore`